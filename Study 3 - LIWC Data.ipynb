{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9c9799",
   "metadata": {},
   "source": [
    "# Study 3: LIWC Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "6935e472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading necessary packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "pd.set_option('display.max_rows', 100, 'display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93809ef-8431-4ae3-8a63-743841f18780",
   "metadata": {},
   "source": [
    "### Steps\n",
    "\n",
    "1. Loading in LIWC Data and Preprocessing\n",
    "2. Merging LIWC Data with Relevant Survey Data from Previous Step\n",
    "3. Calculating non LIWC scores via TextBlob (sentiment and subjectivity)\n",
    "4. Computing Linguistic SImilarity to the Preferred Message from the Message Choice Task\n",
    "5. Melting LIWC dataframe from Wide to Long Format (DF1: Message Preference Outcome) for Mixed Level Analysis\n",
    "6. Melting LIWC dataframe from Wide to Long Format (DF2 & 3: Perceived Personalization and Perceived Message Effectiveness Outcome) for Mixed Level Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeae9eb",
   "metadata": {},
   "source": [
    "# 1. LIWC Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd75df9a-54c9-440b-8d04-c920ebb8de8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5da26628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    }
   ],
   "source": [
    "# Reading in the LIWC results that were run on the exporte Chat Data\n",
    "results_liwc = pd.read_csv('C:/Users/77197jsc/OneDrive - Erasmus University Rotterdam/Documents/Study 3/Data/whatsdata_january_2024/liwc_final2.csv')\n",
    "all_schools_final2 = pd.read_csv('C:/Users/77197jsc/OneDrive - Erasmus University Rotterdam/Documents/Study 3/Data/whatsdata_january_2024/all_schools2.csv')\n",
    "print(len(results_liwc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c473c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movez_code</th>\n",
       "      <th>Message</th>\n",
       "      <th>Number of messages</th>\n",
       "      <th>No_char</th>\n",
       "      <th>No_words</th>\n",
       "      <th>Message_en</th>\n",
       "      <th>Message_checked</th>\n",
       "      <th>School_mail</th>\n",
       "      <th>Segment</th>\n",
       "      <th>WPS</th>\n",
       "      <th>...</th>\n",
       "      <th>want</th>\n",
       "      <th>acquire</th>\n",
       "      <th>lack</th>\n",
       "      <th>fulfill</th>\n",
       "      <th>fatigue</th>\n",
       "      <th>reward</th>\n",
       "      <th>risk</th>\n",
       "      <th>curiosity</th>\n",
       "      <th>allure</th>\n",
       "      <th>Conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Maar voel weer goed dus kan miegen weer pesten...</td>\n",
       "      <td>26</td>\n",
       "      <td>2098</td>\n",
       "      <td>389</td>\n",
       "      <td>But feel good again so can bully miegen again;...</td>\n",
       "      <td>But feel good again so can bully miegen again;...</td>\n",
       "      <td>d.holterman@wpkeesboeke.nl</td>\n",
       "      <td>1</td>\n",
       "      <td>122.67</td>\n",
       "      <td>...</td>\n",
       "      <td>1.36</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.33</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1049218.0</td>\n",
       "      <td>Je ben al 00 min aan het kakken; Broer schiet ...</td>\n",
       "      <td>551</td>\n",
       "      <td>4494</td>\n",
       "      <td>794</td>\n",
       "      <td>Hahahaha isg; Lekka; You've been pooping for 0...</td>\n",
       "      <td>Hahahaha isg; Lekka; You've been pooping for 0...</td>\n",
       "      <td>l113319@gsr.nl</td>\n",
       "      <td>1</td>\n",
       "      <td>60.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.24</td>\n",
       "      <td>2.62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movez_code                                            Message  \\\n",
       "0   1016110.0  Maar voel weer goed dus kan miegen weer pesten...   \n",
       "1   1049218.0  Je ben al 00 min aan het kakken; Broer schiet ...   \n",
       "\n",
       "   Number of messages  No_char  No_words  \\\n",
       "0                  26     2098       389   \n",
       "1                 551     4494       794   \n",
       "\n",
       "                                          Message_en  \\\n",
       "0  But feel good again so can bully miegen again;...   \n",
       "1  Hahahaha isg; Lekka; You've been pooping for 0...   \n",
       "\n",
       "                                     Message_checked  \\\n",
       "0  But feel good again so can bully miegen again;...   \n",
       "1  Hahahaha isg; Lekka; You've been pooping for 0...   \n",
       "\n",
       "                  School_mail  Segment     WPS  ...  want  acquire  lack  \\\n",
       "0  d.holterman@wpkeesboeke.nl        1  122.67  ...  1.36     0.27  0.54   \n",
       "1              l113319@gsr.nl        1   60.00  ...  0.24     0.36  0.00   \n",
       "\n",
       "   fulfill  fatigue  reward  risk  curiosity  allure  Conversation  \n",
       "0     0.00     0.27     0.0  0.00        0.0   10.33          3.80  \n",
       "1     0.12     0.00     0.0  0.12        0.0   10.24          2.62  \n",
       "\n",
       "[2 rows x 29 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets have a look into the data\n",
    "results_liwc.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfb607e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Movez_code', 'Message', 'Number of messages', 'No_char', 'No_words',\n",
      "       'Message_en', 'Message_checked', 'School_mail', 'Segment', 'WPS',\n",
      "       'BigWords', 'Drives', 'cogproc', 'tone_pos', 'tone_neg', 'emo_pos',\n",
      "       'emo_neg', 'Social', 'need', 'want', 'acquire', 'lack', 'fulfill',\n",
      "       'fatigue', 'reward', 'risk', 'curiosity', 'allure', 'Conversation'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(results_liwc.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "555b21c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3629\n"
     ]
    }
   ],
   "source": [
    "#First we are transforming the DF into long format\n",
    "\n",
    "liwc_long = pd.melt(results_liwc, id_vars=['Movez_code', 'Message', 'No_char', \"No_words\", 'Number of messages', 'Message_en',\n",
    "       'Segment',], value_vars=['BigWords', 'Drives', 'cogproc', 'tone_pos',\n",
    "       'tone_neg', 'emo_pos', 'emo_neg', 'Social', 'need', 'want', 'acquire',\n",
    "       'lack', 'fulfill', 'fatigue', 'reward', 'risk', 'curiosity', 'allure',\n",
    "       'Conversation'], var_name = \"Message_pref\", value_name=\"LIWC\")\n",
    "\n",
    "print(len(liwc_long))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a713b352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating standardized values for all LIWC scores\n",
    "\n",
    "# List of columns to standardize\n",
    "cols_to_standardize = [\n",
    "    \"BigWords\", \"Drives\", \"cogproc\", \"tone_pos\", \"tone_neg\", \"emo_pos\", \"emo_neg\",\n",
    "    \"Social\", \"need\", \"want\", \"acquire\", \"lack\", \"fulfill\", \"fatigue\",\n",
    "    \"curiosity\", \"reward\", \"risk\", \"allure\", \"Conversation\"\n",
    "]\n",
    "\n",
    "# Apply standardization\n",
    "for col in cols_to_standardize:\n",
    "    z_col = f\"{col}_z\"\n",
    "    results_liwc[z_col] = (results_liwc[col] - results_liwc[col].mean()) / results_liwc[col].std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b96657c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#From there we can create a new variable psycholinguistic similarity. For this we substract aprticipants LIWC score from the LIWC score of the experimental message\n",
    "\n",
    "# Experimental message LIWC scores\n",
    "experimental_scores = {\n",
    "    \"Drives\": 15.38,\n",
    "    \"cogproc\": 15.38,\n",
    "    \"tone_pos\": 16.00,\n",
    "    \"tone_neg\": 16.67,\n",
    "    \"emo_pos\": 16.00,\n",
    "    \"emo_neg\": 16.00,\n",
    "    \"Social\": 14.81,\n",
    "    \"need\": 14.81,\n",
    "    \"want\": 15.38,\n",
    "    \"acquire\": 15.38,\n",
    "    \"lack\": 14.81,\n",
    "    \"fulfill\": 16.00,\n",
    "    \"fatigue\": 14.81,\n",
    "    \"reward\": 14.81,\n",
    "    \"risk\": 15.38,\n",
    "    \"curiosity\": 16.00,\n",
    "    \"allure\": 16.00,\n",
    "    \"WPS\": 43.00,\n",
    "    \"Conversation\": 14.81,\n",
    "    \"BigWords\": 21.74,\n",
    "}\n",
    "\n",
    "# Compute match scores\n",
    "for feature, exp_value in experimental_scores.items():\n",
    "    results_liwc[f\"match_{feature}\"] = exp_value - results_liwc[feature]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea8e464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets also create a psycholingustic similarity variable in reference to the control base message\n",
    "\n",
    "# Control message LIWC scores\n",
    "control_scores = {\n",
    "    \"Drives\": 0,\n",
    "    \"cogproc\": 0,\n",
    "    \"tone_pos\": 0,\n",
    "    \"tone_neg\": 0,\n",
    "    \"emo_pos\": 0,\n",
    "    \"emo_neg\": 0,\n",
    "    \"Social\": 0,\n",
    "    \"need\": 0,\n",
    "    \"want\": 0,\n",
    "    \"acquire\": 3.85,\n",
    "    \"lack\": 0,\n",
    "    \"fulfill\": 0,\n",
    "    \"fatigue\": 0,\n",
    "    \"reward\": 0,\n",
    "    \"risk\": 0,\n",
    "    \"curiosity\": 0,\n",
    "    \"allure\": 0,\n",
    "    \"WPS\": 8.33,\n",
    "    \"Conversation\": 0,\n",
    "    \"BigWords\": 0,\n",
    "}\n",
    "\n",
    "# Compute control match scores\n",
    "for feature, ctrl_value in control_scores.items():\n",
    "    col_to_use = \"tone_pos\" if feature == \"intensity\" else feature\n",
    "    results_liwc[f\"match_{feature}_ctr\"] = ctrl_value - results_liwc[col_to_use]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e13dd098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\77197jsc\\AppData\\Local\\Temp\\ipykernel_13784\\2450183930.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  results_liwc[f\"ctrmsg_{feature}\"] = val\n",
      "C:\\Users\\77197jsc\\AppData\\Local\\Temp\\ipykernel_13784\\2450183930.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  results_liwc[f\"ctrmsg_{feature}\"] = val\n",
      "C:\\Users\\77197jsc\\AppData\\Local\\Temp\\ipykernel_13784\\2450183930.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  results_liwc[f\"ctrmsg_{feature}\"] = val\n",
      "C:\\Users\\77197jsc\\AppData\\Local\\Temp\\ipykernel_13784\\2450183930.py:26: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  results_liwc[f\"ctrmsg_{feature}\"] = val\n"
     ]
    }
   ],
   "source": [
    "#Lets assign the Message LIWC Scores to the Dataframe as a Reference\n",
    "\n",
    "# Experimental message LIWC values\n",
    "expmsg_values = {\n",
    "    \"Drives\": 15.38, \"cogproc\": 15.38, \"tone_pos\": 16, \"tone_neg\": 16.67, \"emo_pos\": 16,\n",
    "    \"emo_neg\": 16, \"Social\": 14.81, \"need\": 14.81, \"want\": 15.38, \"acquire\": 15.38,\n",
    "    \"lack\": 14.81, \"fulfill\": 16.00, \"fatigue\": 14.81, \"reward\": 14.81, \"risk\": 15.38,\n",
    "    \"curiosity\": 16, \"allure\": 16, \"WPS\": 14.33, \"Conversation\": 14.81, \"BigWords\": 21.74,\n",
    "    \"intensity\": 20.00\n",
    "}\n",
    "\n",
    "# Control message LIWC values\n",
    "ctrmsg_values = {\n",
    "    \"Drives\": 0, \"cogproc\": 0, \"tone_pos\": 0, \"tone_neg\": 0, \"emo_pos\": 0,\n",
    "    \"emo_neg\": 0, \"Social\": 0, \"need\": 0, \"want\": 0, \"acquire\": 3.85,\n",
    "    \"lack\": 0, \"fulfill\": 0, \"fatigue\": 0, \"reward\": 0, \"risk\": 0,\n",
    "    \"curiosity\": 0, \"allure\": 0, \"WPS\": 8.33, \"Conversation\": 0, \"BigWords\": 0,\n",
    "    \"intensity\": 7.69\n",
    "}\n",
    "\n",
    "# Assign values to DataFrame columns\n",
    "for feature, val in expmsg_values.items():\n",
    "    results_liwc[f\"expmsg_{feature}\"] = val\n",
    "\n",
    "for feature, val in ctrmsg_values.items():\n",
    "    results_liwc[f\"ctrmsg_{feature}\"] = val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bdb6a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WPS: 79.32\n",
      "BigWords: 10.35\n",
      "Drives: 3.36\n",
      "cogproc: 11.69\n",
      "tone_pos: 3.19\n",
      "tone_neg: 0.96\n",
      "emo_pos: 0.92\n",
      "emo_neg: 0.49\n",
      "Social: 11.88\n",
      "need: 1.16\n",
      "want: 0.53\n",
      "acquire: 0.76\n",
      "lack: 0.29\n",
      "fulfill: 0.05\n",
      "fatigue: 0.07\n",
      "curiosity: 0.18\n",
      "reward: 0.06\n",
      "risk: 0.11\n",
      "allure: 9.61\n",
      "Conversation: 3.18\n"
     ]
    }
   ],
   "source": [
    "# List of LIWC features to summarize\n",
    "liwc_features = [\n",
    "    \"WPS\", \"BigWords\", \"Drives\", \"cogproc\", \"tone_pos\", \"tone_neg\", \"emo_pos\", \"emo_neg\",\n",
    "    \"Social\", \"need\", \"want\", \"acquire\", \"lack\", \"fulfill\", \"fatigue\", \"curiosity\",\n",
    "    \"reward\", \"risk\", \"allure\", \"Conversation\"\n",
    "]\n",
    "\n",
    "# Print mean values so that we can filter out categories that dont appear often in our data\n",
    "for feature in liwc_features:\n",
    "    print(f\"{feature}: {results_liwc[feature].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63192f9-6ce0-4f62-a2e9-0f070b1f1e55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "346862dc-c6ca-49e1-a80e-5b92c31a7df8",
   "metadata": {},
   "source": [
    "# 2. Merging LIWC Data With Survey Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ce75924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    }
   ],
   "source": [
    "# Finally we can merge the school dataframe with the LIWC dataframe\n",
    "results_liwc[\"Movez_code\"] = results_liwc[\"Movez_code\"].astype(float)\n",
    "all_schools_final2[\"Movez_code\"] = all_schools_final2[\"Movez_code\"].astype(float)\n",
    "\n",
    "# Merging on either 'Movez_code' or 'School_mail'\n",
    "complete_df = pd.merge(all_schools_final2, results_liwc, how='inner', on='School_mail')\n",
    "\n",
    "print(len(complete_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae21e094-711f-4d48-8b44-3d2cd277b7c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['StartDate_left', 'EndDate_left', 'Status_left', 'Progress_left',\n",
       "       'Duration (in seconds)_left', 'Finished_left', 'RecordedDate_left',\n",
       "       'ResponseId_left', 'DistributionChannel_left', 'UserLanguage_left',\n",
       "       ...\n",
       "       'need', 'lack', 'fatigue', 'Drives', 'curiosity', 'Conversation',\n",
       "       'fulfill', 'intensity', 'Pref_value_mean', 'Similarity_mean'],\n",
       "      dtype='object', length=200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_schools_final2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b252ce0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets rename some column names\n",
    "\n",
    "complete_df[\"intensity_y\"] = complete_df[\"tone_pos_y\"]\n",
    "complete_df = complete_df.rename(columns={\"intensity\": \"intensity_x\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a596f4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finally i will compute a Mean LIWC score across all categories\n",
    "\n",
    "complete_df[\"Message_choice_mean\"] = complete_df[[\"cogproc_x\", \"emo_pos_x\", \"emo_neg_x\", \"allure_x\", \"acquire_x\", \"BigWords_x\", \"want_x\", \"WPS_x\", \"Social_x\", \"risk_x\", \"reward_x\", \"tone_pos_x\", \"tone_neg_x\", \"need_x\", \"lack_x\", \"fatigue_x\", \"Drives_x\", \"curiosity_x\", \"Conversation_x\", \"fulfill_x\", \"intensity_x\"]].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e441a62e-389d-4b18-98e3-90f3e9785d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7c3b91f-ac5b-45b0-b9e6-2e8b82e16077",
   "metadata": {},
   "source": [
    "## 3. Computing the non-LIWC Subjectivity and Sentiment Score from TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c62de1e4-1da6-49b2-83e2-1de47f88326e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textblob in c:\\users\\77197jsc\\appdata\\local\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: nltk>=3.9 in c:\\users\\77197jsc\\appdata\\local\\anaconda3\\lib\\site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\77197jsc\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\77197jsc\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\77197jsc\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\77197jsc\\appdata\\local\\anaconda3\\lib\\site-packages (from nltk>=3.9->textblob) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\77197jsc\\appdata\\local\\anaconda3\\lib\\site-packages (from click->nltk>=3.9->textblob) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install textblob\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77e0f150",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_scores(text):\n",
    "    blob = TextBlob(text)\n",
    "    return pd.Series({'Intensity': blob.sentiment.polarity, 'Subjectivity': blob.sentiment.subjectivity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9216d9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df[['Sentiment', 'Subjectivity']] = complete_df['Message_checked'].apply(get_sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae4ce1a-ea18-462a-a081-e6cd29fbe562",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fdfcefe-934e-4cad-818f-37bfb4265930",
   "metadata": {},
   "source": [
    "## 4. Computing Psycholinguistic Similarity to the Preferred Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13ee175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# List of column names to process\n",
    "columns = [\n",
    "    \"cogproc\", \"emo_pos\", \"emo_neg\", \"allure\", \"acquire\", \"BigWords\", \"want\", \"WPS\",\n",
    "    \"Social\", \"risk\", \"reward\", \"tone_pos\", \"tone_neg\", \"need\", \"lack\", \"fatigue\",\n",
    "    \"Drives\", \"curiosity\", \"Conversation\", \"fulfill\", \"intensity\"\n",
    "]\n",
    "\n",
    "# Loop through each column to compute the new values\n",
    "for col in columns:\n",
    "    # Construct the column names for the experimental and control messages\n",
    "    expmsg_col = f\"expmsg_{col}\"\n",
    "    ctrmsg_col = f\"ctrmsg_{col}\"\n",
    "    original_col = f\"{col}_x\"\n",
    "    LIWC_col = f\"{col}_y\"\n",
    "    \n",
    "    # Define the new column name\n",
    "    new_col = f\"{col}_chosenmatch\"\n",
    "    \n",
    "    # Compute the new column based on the condition\n",
    "    complete_df[new_col] = complete_df.apply(\n",
    "        lambda row: (\n",
    "            abs(row[expmsg_col] - row[LIWC_col]) if row[original_col] == 1 else\n",
    "            abs(row[ctrmsg_col] - row[LIWC_col]) if row[original_col] == 0 else\n",
    "            np.nan\n",
    "        ),\n",
    "        axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99a1f5fd-f828-4f03-a11d-940ff79d7bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compute a mean score for all preferred messages across categories\n",
    "\n",
    "complete_df[\"Chosen_message_mean\"] = complete_df[[ 'cogproc_chosenmatch', 'emo_pos_chosenmatch','emo_neg_chosenmatch','allure_chosenmatch','acquire_chosenmatch','BigWords_chosenmatch','want_chosenmatch','WPS_chosenmatch','Social_chosenmatch','risk_chosenmatch','reward_chosenmatch','tone_pos_chosenmatch','tone_neg_chosenmatch','need_chosenmatch','lack_chosenmatch','fatigue_chosenmatch','Drives_chosenmatch','curiosity_chosenmatch','Conversation_chosenmatch','fulfill_chosenmatch','intensity_chosenmatch']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1951e390",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\77197jsc\\AppData\\Local\\Temp\\ipykernel_13784\\2991579992.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  complete_df_short[\"Chosen_message_mean100\"] = 100 - complete_df_short[\"Chosen_message_mean\"]\n"
     ]
    }
   ],
   "source": [
    "#Lets make the df shorter an only keep relevant categories\n",
    "complete_df_short = complete_df[[\"Movez_code_x\", \"Sex\", \"Gender\", \"age\", \"FAS\", \"Message_choice_mean\", \"Chosen_message_mean\", \"Similarity_mean\"]]\n",
    "\n",
    "# We then calculate a similarity score substracted from 100 to increase interpretabiltiy\n",
    "complete_df_short[\"Chosen_message_mean100\"] = 100 - complete_df_short[\"Chosen_message_mean\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56007d2c-01c2-4562-a665-86c760d54100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\77197jsc\\AppData\\Local\\Temp\\ipykernel_13784\\3481665684.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  complete_df_short['Sex'] = complete_df_short['Sex'].map({'Vrouw': 2, 'Man': 1}).fillna(0).astype(int)\n"
     ]
    }
   ],
   "source": [
    "complete_df_short['Sex'] = complete_df_short['Sex'].map({'Vrouw': 2, 'Man': 1}).fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54be5358",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_df_short.to_csv(\"complete_wide_small.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16346044-d40f-43e2-b807-503e5ced3772",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "797f51ec-2746-47a2-81df-a47484b85570",
   "metadata": {},
   "source": [
    "## 5. Melting complete LIWC dataframe from Wide to Long Format (Message Preference Outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1f12189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered value_vars (columns to melt): 20\n",
      "📊 Melted dataframe shape: (3420, 7)\n",
      "⚠️ Rows with no matched category: 0\n",
      "⚠️ Duplicate participant-category combinations before pivot: 20\n",
      "✅ Final pivoted shape: (3400, 3)\n",
      "✅ Final pivoted shape: (3400, 3)\n"
     ]
    }
   ],
   "source": [
    "# Define the list of linguistic categories (original columns)\n",
    "columns = [\n",
    "    \"cogproc\", \"emo_pos\", \"emo_neg\", \"allure\", \"acquire\", \"BigWords\", \"want\", \"WPS\",\n",
    "    \"Social\", \"risk\", \"reward\", \"tone_pos\", \"tone_neg\", \"need\", \"lack\", \"fatigue\",\n",
    "    \"Drives\", \"curiosity\", \"Conversation\", \"fulfill\", \"intensity\"\n",
    "]\n",
    "\n",
    "# Prefix and participant ID columns\n",
    "\n",
    "prefix = \"match_\"\n",
    "id_columns = [\"Movez_code_x\", \"Sex\", \"Gender\", \"age\", \"FAS\"]\n",
    "\n",
    "# Step 1: Filter for columns that match exact prefix + category, and not ending in _ctr\n",
    "value_vars = [\n",
    "    col for col in complete_df.columns\n",
    "    for cat in columns\n",
    "    if col.startswith(f\"{prefix}{cat}\") and not col.endswith(\"_ctr\")\n",
    "]\n",
    "\n",
    "# Remove accidental duplicates\n",
    "value_vars = list(set(value_vars))\n",
    "\n",
    "print(f\"✅ Filtered value_vars (columns to melt): {len(value_vars)}\")\n",
    "# Optional: print(value_vars)\n",
    "\n",
    "# Step 2: Melt dataframe\n",
    "melted = complete_df.melt(\n",
    "    id_vars=id_columns,\n",
    "    value_vars=value_vars,\n",
    "    var_name=\"column\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "print(f\"📊 Melted dataframe shape: {melted.shape}\")  # Expecting ~171*21 = 3591\n",
    "\n",
    "# Step 3: Extract linguistic category\n",
    "def extract_category(col_name):\n",
    "    for cat in columns:\n",
    "        if col_name.startswith(f\"{prefix}{cat}\"):\n",
    "            return cat\n",
    "    return None\n",
    "\n",
    "melted[\"Linguistic Category\"] = melted[\"column\"].apply(extract_category)\n",
    "\n",
    "# Step 4: Check for unmatched categories\n",
    "missing = melted[melted[\"Linguistic Category\"].isna()]\n",
    "print(f\"⚠️ Rows with no matched category: {len(missing)}\")\n",
    "if not missing.empty:\n",
    "    print(missing.head())\n",
    "\n",
    "# Step 5: Check for duplicates before pivot\n",
    "dupes = melted.duplicated(subset=id_columns + [\"Linguistic Category\"])\n",
    "print(f\"⚠️ Duplicate participant-category combinations before pivot: {dupes.sum()}\")\n",
    "\n",
    "# Step 6: Pivot table using participant ID only (avoid accidental over-indexing)\n",
    "pivoted = melted.pivot_table(\n",
    "    index=[\"Movez_code_x\", \"Linguistic Category\"],\n",
    "    values=\"value\",\n",
    "    aggfunc=\"first\",\n",
    "    dropna=False\n",
    ").reset_index()\n",
    "\n",
    "pivoted.columns.name = None\n",
    "\n",
    "print(f\"✅ Final pivoted shape: {pivoted.shape}\")  # Should be (171*21 = 3591, 3)\n",
    "\n",
    "pivoted.columns.name = None\n",
    "\n",
    "print(f\"✅ Final pivoted shape: {pivoted.shape}\")  # Should be 171 * 21 = 3591\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da97767a-ca9a-4fdd-8aef-a92b978e89f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Filtered value_vars (columns to melt): 21\n",
      "📊 Melted dataframe shape: (3591, 7)\n",
      "⚠️ Rows with no matched category: 0\n",
      "⚠️ Duplicate participant-category combinations before pivot: 21\n",
      "✅ Final pivoted shape: (3570, 3)\n"
     ]
    }
   ],
   "source": [
    "# Define the list of linguistic categories (original columns)\n",
    "columns = [\n",
    "    \"cogproc\", \"emo_pos\", \"emo_neg\", \"allure\", \"acquire\", \"BigWords\", \"want\", \"WPS\",\n",
    "    \"Social\", \"risk\", \"reward\", \"tone_pos\", \"tone_neg\", \"need\", \"lack\", \"fatigue\",\n",
    "    \"Drives\", \"curiosity\", \"Conversation\", \"fulfill\", \"intensity\"\n",
    "]\n",
    "\n",
    "# Suffix and participant ID columns\n",
    "suffix = \"_x\"\n",
    "id_columns = [\"Movez_code_x\", \"Sex\", \"Gender\", \"age\", \"FAS\"]\n",
    "\n",
    "# Step 1: Filter columns that match exact category + suffix, and not ending in '_ctr'\n",
    "value_vars = [\n",
    "    col for col in complete_df.columns\n",
    "    if any(col == f\"{cat}{suffix}\" for cat in columns) and not col.endswith(\"_ctr\")\n",
    "]\n",
    "\n",
    "# Remove duplicates (just in case)\n",
    "value_vars = list(set(value_vars))\n",
    "\n",
    "print(f\"✅ Filtered value_vars (columns to melt): {len(value_vars)}\")\n",
    "# Optional: print(value_vars)\n",
    "\n",
    "# Step 2: Melt dataframe\n",
    "melted2 = complete_df.melt(\n",
    "    id_vars=id_columns,\n",
    "    value_vars=value_vars,\n",
    "    var_name=\"column\",\n",
    "    value_name=\"value\"\n",
    ")\n",
    "\n",
    "print(f\"📊 Melted dataframe shape: {melted2.shape}\")  # Expecting ~171*21 = 3591\n",
    "\n",
    "# Step 3: Extract linguistic category\n",
    "def extract_category(col_name):\n",
    "    for cat in columns:\n",
    "        if col_name == f\"{cat}{suffix}\":\n",
    "            return cat\n",
    "    return None\n",
    "\n",
    "melted2[\"Linguistic Category\"] = melted2[\"column\"].apply(extract_category)\n",
    "\n",
    "# Step 4: Check for unmatched categories\n",
    "missing = melted2[melted2[\"Linguistic Category\"].isna()]\n",
    "print(f\"⚠️ Rows with no matched category: {len(missing)}\")\n",
    "if not missing.empty:\n",
    "    print(missing.head())\n",
    "\n",
    "# Step 5: Check for duplicates before pivot\n",
    "dupes = melted2.duplicated(subset=id_columns + [\"Linguistic Category\"])\n",
    "print(f\"⚠️ Duplicate participant-category combinations before pivot: {dupes.sum()}\")\n",
    "\n",
    "# Step 6: Pivot table using participant ID only\n",
    "pivoted2 = melted2.pivot_table(\n",
    "    index=[\"Movez_code_x\", \"Linguistic Category\"],\n",
    "    values=\"value\",\n",
    "    aggfunc=\"first\",\n",
    "    dropna=False\n",
    ").reset_index()\n",
    "\n",
    "pivoted2.columns.name = None\n",
    "\n",
    "print(f\"✅ Final pivoted shape: {pivoted2.shape}\")  # Should be 171 * 21 = 3591\n",
    "\n",
    "message_pref = pivoted2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c0e3c6d-181c-4b49-842e-781010b158b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some renaming of columns\n",
    "\n",
    "pivoted = pivoted.rename(columns={\"value\": \"Similarity: EXP\"}) \n",
    "message_pref = message_pref.rename(columns={\"value\": \"Message_pref\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0733999d-981e-4293-8bef-fd6c8a7af357",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linguistic Category\n",
       "BigWords        170\n",
       "Conversation    170\n",
       "tone_pos        170\n",
       "tone_neg        170\n",
       "risk            170\n",
       "reward          170\n",
       "need            170\n",
       "lack            170\n",
       "fulfill         170\n",
       "fatigue         170\n",
       "emo_pos         170\n",
       "emo_neg         170\n",
       "curiosity       170\n",
       "cogproc         170\n",
       "allure          170\n",
       "acquire         170\n",
       "WPS             170\n",
       "Social          170\n",
       "Drives          170\n",
       "want            170\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now we can have a look into the data structure\n",
    "\n",
    "pivoted[\"Linguistic Category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d8e539e9-7709-4f4e-8fbc-86e29b605bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movez_code_x</th>\n",
       "      <th>Linguistic Category</th>\n",
       "      <th>Message_pref</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>BigWords</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Conversation</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Drives</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Social</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>WPS</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movez_code_x Linguistic Category  Message_pref\n",
       "0     1016110.0            BigWords           0.0\n",
       "1     1016110.0        Conversation           0.0\n",
       "2     1016110.0              Drives           0.0\n",
       "3     1016110.0              Social           1.0\n",
       "4     1016110.0                 WPS           0.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the particiaptn information\n",
    "message_pref.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf29d72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3400\n",
      "3400\n"
     ]
    }
   ],
   "source": [
    "# Lets also gather the participant-level information into one dataframe\n",
    "\n",
    "# Step 1: Get participant-level info\n",
    "participant_info = complete_df[[\"Movez_code_x\", \"Sex\", \"Gender\", \"age\", \"FAS\"]].drop_duplicates()\n",
    "\n",
    "# Step 2: Merge demographics directly into the long-format dataframe\n",
    "pivoted_with_demo1 = pd.merge(pivoted, participant_info, on=\"Movez_code_x\", how=\"left\")\n",
    "print(len(pivoted_with_demo1))\n",
    "pivoted_with_demo2 = pd.merge(pivoted_with_demo1, message_pref, on=[\"Movez_code_x\", \"Linguistic Category\"], how=\"left\")\n",
    "print(len(pivoted_with_demo2))\n",
    "\n",
    "# Again we will compute a score relative to 100 for similarity\n",
    "pivoted_with_demo2[\"Similarity: EXP_100\"] = 100 - pivoted_with_demo2[\"Similarity: EXP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de038bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movez_code_x</th>\n",
       "      <th>Linguistic Category</th>\n",
       "      <th>Similarity: EXP</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Gender</th>\n",
       "      <th>age</th>\n",
       "      <th>FAS</th>\n",
       "      <th>Message_pref</th>\n",
       "      <th>Similarity: EXP_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>BigWords</td>\n",
       "      <td>13.32</td>\n",
       "      <td>Man</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>86.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Conversation</td>\n",
       "      <td>11.01</td>\n",
       "      <td>Man</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Drives</td>\n",
       "      <td>11.30</td>\n",
       "      <td>Man</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Social</td>\n",
       "      <td>3.67</td>\n",
       "      <td>Man</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>96.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>WPS</td>\n",
       "      <td>-79.67</td>\n",
       "      <td>Man</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>179.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movez_code_x Linguistic Category  Similarity: EXP  Sex Gender   age  \\\n",
       "0     1016110.0            BigWords            13.32  Man    Man  14.0   \n",
       "1     1016110.0        Conversation            11.01  Man    Man  14.0   \n",
       "2     1016110.0              Drives            11.30  Man    Man  14.0   \n",
       "3     1016110.0              Social             3.67  Man    Man  14.0   \n",
       "4     1016110.0                 WPS           -79.67  Man    Man  14.0   \n",
       "\n",
       "        FAS  Message_pref  Similarity: EXP_100  \n",
       "0  1.833333           0.0                86.68  \n",
       "1  1.833333           0.0                88.99  \n",
       "2  1.833333           0.0                88.70  \n",
       "3  1.833333           1.0                96.33  \n",
       "4  1.833333           0.0               179.67  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets have a look into the df with demographic information merged\n",
    "pivoted_with_demo2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bfe18042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pivoted_with_demo2.to_csv(\"complete_df_EXPSIM4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a29909a9-0209-4953-9ac9-6d1dfe47caf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#complete_df[\"Pref_value\"] = complete_df[[\"cogproc\", \"emo_pos\", \"emo_neg\", \"allure\", \"acquire\", \"BigWords\", \"want\", \"subj\", \"WPS\", \"Social\", \"risk\", \"reward\", \"tone_pos\", \"tone_neg\", \"need\", \"lack\", \"fatigue\", \"Drives\", \"curiosity\", \"Conversation\", \"fulfill\"]].mean(axis=1)\n",
    "#complete_df.to_csv(\"complete_wide.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae1b31c-f0b0-4106-b00a-c7150a9c32f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0798f353-6b9c-4cc6-9d12-9129541b66aa",
   "metadata": {},
   "source": [
    "## 6. Melting complete LIWC dataframe from Wide to Long Format (PEM and PP Outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1d3d23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df_eff = pd.melt(complete_df, id_vars=['Movez_code_x', \"Sex\", \"age\", \"FAS\", \"Gender\"], value_vars = ['eff_WPS', 'eff_acquire', 'eff_allure', 'eff_bigwords', 'eff_cog', 'eff_convers', 'eff_curious', 'eff_drive', 'eff_fatigue', 'eff_fulfill', 'eff_intensity', 'eff_lack', 'eff_need', 'eff_negemo', 'eff_negtone', 'eff_posemo', 'eff_postone', 'eff_reward', 'eff_risk', 'eff_social', 'eff_subj', 'eff_want'],var_name= \"Linguistic category\", value_name='Eff_value')\n",
    "complete_df_eff[\"Linguistic category\"] = complete_df_eff[\"Linguistic category\"].replace(['eff_WPS', 'eff_acquire', 'eff_allure', 'eff_bigwords', 'eff_cog', 'eff_convers', 'eff_curious', 'eff_drive', 'eff_fatigue', 'eff_fulfill', 'eff_intensity', 'eff_lack', 'eff_need', 'eff_negemo', 'eff_negtone', 'eff_posemo', 'eff_postone', 'eff_reward', 'eff_risk', 'eff_social', 'eff_subj', 'eff_want'], ['WPS', 'acquire', 'allure', 'BigWords', \"cogproc\", 'Conversation', 'curiosity', 'Drives', 'fatigue', \"fulfill\", 'intensity', 'lack', 'need', 'emo_neg', 'tone_neg', 'emo_pos', 'tone_pos', 'reward', 'risk', \"Social\", \"subj\", \"want\"])\n",
    "complete_df_eff = complete_df_eff[complete_df_eff[\"Movez_code_x\"].isna() == False]\n",
    "complete_df_eff = complete_df_eff[complete_df_eff[\"Eff_value\"].isna() == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5f42f44-baf6-44a6-ac6d-338979889cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movez_code_x</th>\n",
       "      <th>Sex</th>\n",
       "      <th>age</th>\n",
       "      <th>FAS</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Linguistic category</th>\n",
       "      <th>Eff_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3504199.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>WPS</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9089051.0</td>\n",
       "      <td>Vrouw</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Vrouw</td>\n",
       "      <td>WPS</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>6989421.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>WPS</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1054448.0</td>\n",
       "      <td>Vrouw</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Vrouw</td>\n",
       "      <td>WPS</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2141126.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>WPS</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Movez_code_x    Sex   age  FAS Gender Linguistic category  Eff_value\n",
       "30     3504199.0    Man  14.0  2.0    Man                 WPS        2.0\n",
       "40     9089051.0  Vrouw  13.0  1.0  Vrouw                 WPS        1.0\n",
       "46     6989421.0    Man  14.0  1.0    Man                 WPS        1.0\n",
       "51     1054448.0  Vrouw  15.0  1.5  Vrouw                 WPS        4.0\n",
       "56     2141126.0    Man  14.0  2.0    Man                 WPS        3.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df_eff.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "184df318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets rename some colimn names so that they are uniform\n",
    "\n",
    "complete_df = complete_df.rename(columns= {\"pp_bigwords\": 'pp_BigWords', \"pp_convers\": 'pp_Conversation', \"pp_drive\": 'pp_Drives', \"pp_social\": 'pp_Social', \"pp_cog\": 'pp_cogproc', 'pp_curious': 'pp_curiosity', \"pp_negemo\": 'pp_emo_neg', \"pp_posemo\": 'pp_emo_pos', \"pp_negtone\": 'pp_tone_neg', \"pp_postone\": 'pp_tone_pos'})\n",
    "complete_df = complete_df.rename(columns= {\"eff_bigwords\": 'eff_BigWords', \"eff_convers\": 'eff_Conversation', \"eff_drive\": 'eff_Drives', \"eff_social\": 'eff_Social', \"eff_cog\": 'eff_cogproc', 'eff_curious': 'eff_curiosity', \"eff_negemo\": 'eff_emo_neg', \"eff_posemo\": 'eff_emo_pos', \"eff_negtone\": 'eff_tone_neg', \"eff_postone\": 'eff_tone_pos'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "469bbbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again we calcualte linguistic similarity to the Preferred Message\n",
    "\n",
    "# Define linguistic categories of interest\n",
    "columns = [\n",
    "    \"cogproc\", \"emo_pos\", \"emo_neg\", \"allure\", \"acquire\", \"BigWords\", \"want\", \"WPS\",\n",
    "    \"Social\", \"risk\", \"reward\", \"tone_pos\", \"tone_neg\", \"need\", \"lack\", \"fatigue\",\n",
    "    \"Drives\", \"curiosity\", \"Conversation\", \"fulfill\", \"intensity\"\n",
    "]\n",
    "\n",
    "# Define additional columns to retain\n",
    "retain_cols = [\"Movez_code_x\", \"Sex\", \"age\", \"FAS\", \"Gender\"]\n",
    "\n",
    "# Melt eff_ columns (exclude any ending in _z)\n",
    "eff_columns = [\n",
    "    col for col in complete_df.columns \n",
    "    if col.startswith(\"eff_\") \n",
    "]\n",
    "df_eff = complete_df.melt(\n",
    "    id_vars=retain_cols,\n",
    "    value_vars=eff_columns,\n",
    "    var_name=\"Linguistic Category\",\n",
    "    value_name=\"Eff value\"\n",
    ")\n",
    "df_eff[\"Linguistic Category\"] = df_eff[\"Linguistic Category\"].str.replace(\"eff_\", \"\", regex=False)\n",
    "\n",
    "# Melt _chosenmatch columns (exclude any ending in _chosenmatch_z)\n",
    "chosenmatch_columns = [\n",
    "    col for col in complete_df.columns \n",
    "    if col.endswith(\"_chosenmatch\")\n",
    "]\n",
    "df_chosen = complete_df.melt(\n",
    "    id_vars=retain_cols,\n",
    "    value_vars=chosenmatch_columns,\n",
    "    var_name=\"Linguistic Category\",\n",
    "    value_name=\"Similarity: CHOSEN\"\n",
    ")\n",
    "df_chosen[\"Linguistic Category\"] = df_chosen[\"Linguistic Category\"].str.replace(\"_chosenmatch\", \"\", regex=False)\n",
    "\n",
    "# Merge both long-format DataFrames on participant info and variable name\n",
    "df_long_eff = pd.merge(\n",
    "    df_eff,\n",
    "    df_chosen,\n",
    "    on=retain_cols + [\"Linguistic Category\"],\n",
    "    how=\"outer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9316d47-51d2-427f-b44e-0b19ddf37aa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movez_code_x</th>\n",
       "      <th>Sex</th>\n",
       "      <th>age</th>\n",
       "      <th>FAS</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Linguistic Category</th>\n",
       "      <th>Eff value</th>\n",
       "      <th>Similarity: CHOSEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>BigWords</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Conversation</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Drives</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Social</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>WPS</td>\n",
       "      <td>3.0</td>\n",
       "      <td>114.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>acquire</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>allure</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>cogproc</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>curiosity</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>emo_neg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movez_code_x  Sex   age       FAS Gender Linguistic Category  Eff value  \\\n",
       "0     1016110.0  Man  14.0  1.833333    Man            BigWords        NaN   \n",
       "1     1016110.0  Man  14.0  1.833333    Man        Conversation        2.0   \n",
       "2     1016110.0  Man  14.0  1.833333    Man              Drives        3.0   \n",
       "3     1016110.0  Man  14.0  1.833333    Man              Social        NaN   \n",
       "4     1016110.0  Man  14.0  1.833333    Man                 WPS        3.0   \n",
       "5     1016110.0  Man  14.0  1.833333    Man             acquire        NaN   \n",
       "6     1016110.0  Man  14.0  1.833333    Man              allure        NaN   \n",
       "7     1016110.0  Man  14.0  1.833333    Man             cogproc        NaN   \n",
       "8     1016110.0  Man  14.0  1.833333    Man           curiosity        2.0   \n",
       "9     1016110.0  Man  14.0  1.833333    Man             emo_neg        NaN   \n",
       "\n",
       "   Similarity: CHOSEN  \n",
       "0                8.42  \n",
       "1                3.80  \n",
       "2                4.08  \n",
       "3                3.67  \n",
       "4              114.34  \n",
       "5               15.11  \n",
       "6                5.67  \n",
       "7                2.61  \n",
       "8                0.00  \n",
       "9                0.54  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long_eff.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64b44afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We repeat the same thing for Perceived Personalization\n",
    "\n",
    "# Columns to melt\n",
    "columns = [\n",
    "    \"cogproc\", \"emo_pos\", \"emo_neg\", \"allure\", \"acquire\", \"BigWords\", \"want\", \"WPS\",\n",
    "    \"Social\", \"risk\", \"reward\", \"tone_pos\", \"tone_neg\", \"need\", \"lack\", \"fatigue\",\n",
    "    \"Drives\", \"curiosity\", \"Conversation\", \"fulfill\", \"intensity\"\n",
    "]\n",
    "\n",
    "# Additional columns to retain\n",
    "retain_cols = [\"Movez_code_x\", \"Sex\", \"age\", \"FAS\", \"Gender\"]\n",
    "\n",
    "# Melt eff_ columns\n",
    "pp_columns = [col for col in complete_df.columns if col.startswith(\"pp_\")]\n",
    "df_pp = complete_df.melt(\n",
    "    id_vars=retain_cols,\n",
    "    value_vars=pp_columns,\n",
    "    var_name=\"Linguistic Category\",\n",
    "    value_name=\"PP value\"\n",
    ")\n",
    "df_pp[\"Linguistic Category\"] = df_pp[\"Linguistic Category\"].str.replace(\"pp_\", \"\", regex=False)\n",
    "\n",
    "# Melt _chosenmatch_z columns\n",
    "chosenmatch_columns = [col for col in complete_df.columns if col.endswith(\"_chosenmatch\")]\n",
    "df_chosen = complete_df.melt(\n",
    "    id_vars=retain_cols,\n",
    "    value_vars=chosenmatch_columns,\n",
    "    var_name=\"Linguistic Category\",\n",
    "    value_name=\"Similarity: CHOSEN\"\n",
    ")\n",
    "df_chosen[\"Linguistic Category\"] = df_chosen[\"Linguistic Category\"].str.replace(\"_chosenmatch\", \"\", regex=False)\n",
    "\n",
    "# Combine melted DataFrames\n",
    "df_long_pp = pd.merge(df_pp, df_chosen, on=retain_cols + [\"Linguistic Category\"], how=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5dca9b07-6f16-4aad-bcb2-e61e78afdb22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movez_code_x</th>\n",
       "      <th>Sex</th>\n",
       "      <th>age</th>\n",
       "      <th>FAS</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Linguistic Category</th>\n",
       "      <th>PP value</th>\n",
       "      <th>Similarity: CHOSEN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>BigWords</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Conversation</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Drives</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Social</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>WPS</td>\n",
       "      <td>2.0</td>\n",
       "      <td>114.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movez_code_x  Sex   age       FAS Gender Linguistic Category  PP value  \\\n",
       "0     1016110.0  Man  14.0  1.833333    Man            BigWords       NaN   \n",
       "1     1016110.0  Man  14.0  1.833333    Man        Conversation       3.0   \n",
       "2     1016110.0  Man  14.0  1.833333    Man              Drives       4.0   \n",
       "3     1016110.0  Man  14.0  1.833333    Man              Social       NaN   \n",
       "4     1016110.0  Man  14.0  1.833333    Man                 WPS       2.0   \n",
       "\n",
       "   Similarity: CHOSEN  \n",
       "0                8.42  \n",
       "1                3.80  \n",
       "2                4.08  \n",
       "3                3.67  \n",
       "4              114.34  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a295586-d987-4fb9-81bd-ee8f9d890408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Again we transform the scores substracting from 100\n",
    "\n",
    "df_long_pp[\"Similarity: CHOSEN_100\"] = 100 - df_long_pp[\"Similarity: CHOSEN\"]\n",
    "df_long_eff[\"Similarity: CHOSEN_100\"] = 100 - df_long_eff[\"Similarity: CHOSEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "65288326-3cdf-4bfa-a917-ff5ab3f9d40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movez_code_x</th>\n",
       "      <th>Sex</th>\n",
       "      <th>age</th>\n",
       "      <th>FAS</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Linguistic Category</th>\n",
       "      <th>PP value</th>\n",
       "      <th>Similarity: CHOSEN</th>\n",
       "      <th>Similarity: CHOSEN_100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>BigWords</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.42</td>\n",
       "      <td>91.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Conversation</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.80</td>\n",
       "      <td>96.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Drives</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.08</td>\n",
       "      <td>95.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>Social</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.67</td>\n",
       "      <td>96.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1016110.0</td>\n",
       "      <td>Man</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>Man</td>\n",
       "      <td>WPS</td>\n",
       "      <td>2.0</td>\n",
       "      <td>114.34</td>\n",
       "      <td>-14.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Movez_code_x  Sex   age       FAS Gender Linguistic Category  PP value  \\\n",
       "0     1016110.0  Man  14.0  1.833333    Man            BigWords       NaN   \n",
       "1     1016110.0  Man  14.0  1.833333    Man        Conversation       3.0   \n",
       "2     1016110.0  Man  14.0  1.833333    Man              Drives       4.0   \n",
       "3     1016110.0  Man  14.0  1.833333    Man              Social       NaN   \n",
       "4     1016110.0  Man  14.0  1.833333    Man                 WPS       2.0   \n",
       "\n",
       "   Similarity: CHOSEN  Similarity: CHOSEN_100  \n",
       "0                8.42                   91.58  \n",
       "1                3.80                   96.20  \n",
       "2                4.08                   95.92  \n",
       "3                3.67                   96.33  \n",
       "4              114.34                  -14.34  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4f44ddb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_long_eff.to_csv(\"eff_long.csv\")\n",
    "#df_long_pp.to_csv(\"pp_long.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b90f6-f681-409e-b7c3-fb44f81fb8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
